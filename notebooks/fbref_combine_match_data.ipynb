{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from helper import create_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_csv_files(folder_path, file_suffix):\n",
    "    dfs = []\n",
    "    for root, dirs, files in os.walk(folder_path):\n",
    "        for file in files:\n",
    "            if file.endswith(file_suffix):\n",
    "                file_path = os.path.join(root, file)\n",
    "                df = pd.read_csv(file_path)\n",
    "                match_uid = os.path.basename(os.path.dirname(file_path))\n",
    "                df['match_uid'] = str(match_uid)\n",
    "                dfs.append(df)\n",
    "    \n",
    "    combined_df = pd.concat(dfs, axis=0, ignore_index=True)\n",
    "    return combined_df\n",
    "\n",
    "# Combine _outfield.csv files\n",
    "outfield_folder_path = \"match_data\"\n",
    "outfield_suffix = \"_outfield.csv\"\n",
    "outfield_combined_df = combine_csv_files(outfield_folder_path, outfield_suffix)\n",
    "\n",
    "# Combine _keeper.csv files\n",
    "keeper_folder_path = \"match_data\"\n",
    "keeper_suffix = \"_keeper.csv\"\n",
    "keeper_combined_df = combine_csv_files(keeper_folder_path, keeper_suffix)\n",
    "\n",
    "# Save the combined dataframes to CSV files\n",
    "create_folder(\"combined_match_data\")\n",
    "outfield_combined_df.to_csv(\"combined_match_data/outfield_combined.csv\", index=False)\n",
    "keeper_combined_df.to_csv(\"combined_match_data/keeper_combined.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "# Specify the input CSV files\n",
    "outfield_csv = 'combined_match_data/outfield_combined.csv'\n",
    "keeper_csv = 'combined_match_data/keeper_combined.csv'\n",
    "\n",
    "# Specify the output CSV files\n",
    "outfield_processed = 'combined_match_data/outfield_processed.csv'\n",
    "keeper_processed = 'combined_match_data/keeper_processed.csv'\n",
    "\n",
    "# Specify the value to replace empty cells\n",
    "replacement_value = 'NULL'  # or any other value that MySQL Workbench recognizes\n",
    "\n",
    "# Function to process the CSV file\n",
    "def process_csv(input_file, output_file):\n",
    "    with open(input_file, 'r') as file_in, open(output_file, 'w', newline='') as file_out:\n",
    "        reader = csv.reader(file_in)\n",
    "        writer = csv.writer(file_out)\n",
    "\n",
    "        for row in reader:\n",
    "            processed_row = [cell if cell else replacement_value for cell in row]\n",
    "            writer.writerow(processed_row)\n",
    "\n",
    "# Process the outfield CSV\n",
    "process_csv(outfield_csv, outfield_processed)\n",
    "\n",
    "# Process the keeper CSV\n",
    "process_csv(keeper_csv, keeper_processed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All CSV files processed.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Function to convert to snake_case\n",
    "def convert_to_snake_case(name):\n",
    "    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()\n",
    "\n",
    "# List of CSV files\n",
    "csv_files = [\n",
    "    '/Users/limjohn/Documents/GitHub/fpl-scraping/team_data/team_data_epl_22_23.csv',\n",
    "    '/Users/limjohn/Documents/GitHub/fpl-scraping/match_data/all_match_links_epl_22_23.csv',\n",
    "    '/Users/limjohn/Documents/GitHub/fpl-scraping/player_data/players_epl_22_23.csv',\n",
    "    '/Users/limjohn/Documents/GitHub/fpl-scraping/combined_match_data/keeper_processed.csv',\n",
    "    '/Users/limjohn/Documents/GitHub/fpl-scraping/combined_match_data/outfield_processed.csv'\n",
    "]\n",
    "\n",
    "# Process each CSV file\n",
    "for csv_file in csv_files:\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    # Convert column names to snake_case\n",
    "    df.columns = [convert_to_snake_case(col) for col in df.columns]\n",
    "\n",
    "    # Specific case for 'all_match_links_epl_22_23.csv'\n",
    "    if 'all_match_links_epl_22_23.csv' in csv_file:\n",
    "        df.rename(columns={'x_g': 'xg_home', 'x_g.1': 'xg_away'}, inplace=True)\n",
    "\n",
    "    # Write the data back to the CSV file\n",
    "    df.to_csv(csv_file, index=False)\n",
    "\n",
    "print(\"All CSV files processed.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpl-scraper-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
