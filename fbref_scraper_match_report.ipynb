{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urljoin\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from webdriver_manager.chrome import ChromeDriverManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DRIVER = webdriver.Chrome(service=Service(ChromeDriverManager().install()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "FBREF_HOMEPAGE_URL = \"https://fbref.com/\"\n",
    "PREM_URL = \"https://fbref.com/en/comps/9/Premier-League-Stats\"\n",
    "LEAGUE_TEAM_TABLE_ID = \"results2022-202391_overall\"\n",
    "MATCH_REPORT_URL = \"https://fbref.com/en/matches/dff22d13/Newcastle-United-Tottenham-Hotspur-April-23-2023-Premier-League\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RETRIEVE SCOREBOX INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_scorebox_soup(url, driver, tag_id = \"scorebox\"):\n",
    "    # Load the page using Selenium\n",
    "    driver.get(url)\n",
    "    \n",
    "    wait = WebDriverWait(driver, 10)\n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, tag_id)))\n",
    "\n",
    "    # create a Beautiful Soup object from the response content\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    return soup\n",
    "\n",
    "\n",
    "def get_teams_playing_uids(soup):    \n",
    "    # find the div with id 'scorebox'\n",
    "    div = soup.find('div', {'class': \"scorebox\"})\n",
    "\n",
    "    divs = div.find_all(\"div\", recursive=False, limit=2)\n",
    "    home_team_div, away_team_div = divs[0], divs[1]\n",
    "    home_team_uid = home_team_div.find('a')['href'].split('/')[3]\n",
    "    away_team_uid = away_team_div.find('a')['href'].split('/')[3]\n",
    "\n",
    "    return [home_team_uid, away_team_uid]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RETRIEVE OUTFIELD PLAYER METRICS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_outfield_perf_soup(uids, url, driver):\n",
    "    # Load the page using Selenium\n",
    "    driver.get(url)\n",
    "    \n",
    "    wait = WebDriverWait(driver, 10)    \n",
    "    wait.until(EC.presence_of_element_located((By.ID, f\"stats_{uids[0]}_summary\")))\n",
    "    wait.until(EC.presence_of_element_located((By.ID, f\"stats_{uids[1]}_summary\")))\n",
    "\n",
    "    # create a Beautiful Soup object from the response content\n",
    "    soup = BeautifulSoup(DRIVER.page_source, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_outfield_perf(team_uids, soup):\n",
    "\n",
    "    df_list = []\n",
    "    # find the div with id 'stats_teamuid_summary'\n",
    "    for team_uid in team_uids:\n",
    "        table = soup.find('table', {'id': f\"stats_{team_uid}_summary\"})\n",
    "\n",
    "        data = []\n",
    "        table_headers = table.find_all('th')\n",
    "\n",
    "        # retrive column headers (aka names of the statistics)\n",
    "        first_index_stat_header = 7\n",
    "        last_index_stat_header = 37\n",
    "        col_stats_names = [header.get('aria-label') for header in table_headers][first_index_stat_header:(last_index_stat_header + 1)] # ignore 0-6, take 7-37\n",
    "        col_stats_names.insert(0, \"player_uid\")\n",
    "        col_stats_names.insert(0, \"team_uid\")\n",
    "\n",
    "        # retrieve player names\n",
    "        player_info = table_headers[(last_index_stat_header + 1):-1] # omit the last row since its an aggregation row.\n",
    "        player_names = [player.get_text().strip() for player in player_info]\n",
    "        player_uid = [player.find('a')['href'].split('/')[3] for player in player_info]\n",
    "        \n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        rows = table_body.find_all('tr')\n",
    "        row_index = 0\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            cols.insert(0, player_names[row_index])\n",
    "            cols.insert(0, player_uid[row_index])\n",
    "            cols.insert(0, team_uid)\n",
    "            row_index += 1\n",
    "            data.append(cols)\n",
    "\n",
    "        # Convert data to DataFrame\n",
    "        df = pd.DataFrame(data, columns = col_stats_names)\n",
    "        df_list.append(df)\n",
    "\n",
    "    final_match_df = pd.concat(df_list, ignore_index=True)\n",
    "    return final_match_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RETRIEVE KEEPER METRICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_keeper_perf_soup(uids, url, driver):\n",
    "    # Load the page using Selenium\n",
    "    driver.get(url)\n",
    "    \n",
    "    wait = WebDriverWait(driver, 10)    \n",
    "    wait.until(EC.presence_of_element_located((By.ID, f\"keeper_stats_{uids[0]}\")))\n",
    "    wait.until(EC.presence_of_element_located((By.ID, f\"keeper_stats_{uids[1]}\")))\n",
    "\n",
    "    # create a Beautiful Soup object from the response content\n",
    "    soup = BeautifulSoup(DRIVER.page_source, 'html.parser')\n",
    "    return soup\n",
    "\n",
    "def get_keeper_perf(team_uids, soup):\n",
    "\n",
    "    df_list = []\n",
    "    # find the div with id 'keeper_stats_{team_uid}'\n",
    "    for team_uid in team_uids:\n",
    "        table = soup.find('table', {'id': f\"keeper_stats_{team_uid}\"})\n",
    "\n",
    "        data = []\n",
    "        table_headers = table.find_all('th')\n",
    "\n",
    "        # retrive column headers (aka names of the statistics)\n",
    "        first_index_stat_header = 7\n",
    "        last_index_stat_header = 30\n",
    "        col_stats_names = [header.get('aria-label') for header in table_headers][first_index_stat_header:(last_index_stat_header + 1)] # ignore 0-6, take 7-30\n",
    "        col_stats_names.insert(0, \"player_uid\")\n",
    "        col_stats_names.insert(0, \"team_uid\")\n",
    "\n",
    "        # retrieve player names\n",
    "        player_info = table_headers[(last_index_stat_header + 1):] # retrieve all keepers that played\n",
    "        player_names = [player.get_text().strip() for player in player_info]\n",
    "\n",
    "        table_body = table.find('tbody')\n",
    "\n",
    "        rows = table_body.find_all('tr')\n",
    "        row_index = 0\n",
    "        for row in rows:\n",
    "            cols = row.find_all('td')\n",
    "            player_uid = row.find('th', {'data-stat': \"player\"}).get('data-append-csv').strip()\n",
    "            cols = [ele.text.strip() for ele in cols]\n",
    "            cols.insert(0, player_names[row_index])\n",
    "            cols.insert(0, player_uid)\n",
    "            cols.insert(0, team_uid)\n",
    "            row_index += 1\n",
    "            data.append(cols)\n",
    "\n",
    "        # Convert data to DataFrame\n",
    "        df = pd.DataFrame(data, columns = col_stats_names)\n",
    "        df_list.append(df)\n",
    "\n",
    "    final_match_df = pd.concat(df_list, ignore_index=True)\n",
    "    return final_match_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD MATCH REPORT SOUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_match_soup(url, driver, tag_id = \"scorebox\"):\n",
    "    # Load the page using Selenium\n",
    "    driver.get(url)\n",
    "    \n",
    "    wait = WebDriverWait(driver, 10)    \n",
    "\n",
    "    # waiting till scorebox appears\n",
    "    wait.until(EC.presence_of_element_located((By.CLASS_NAME, tag_id)))\n",
    "\n",
    "    # wait until at least two elements with id starting with 'id_???_summary' are present in the DOM\n",
    "    wait.until(lambda d: len(d.find_elements(By.CSS_SELECTOR, \"[id^='stats_'][id$='_summary']\"))>=2)\n",
    "\n",
    "    # wait until at least two elements with id starting with 'keeper_stats_???' are present in the DOM\n",
    "    wait.until(lambda d: len(d.find_elements(By.CSS_SELECTOR, \"[id^='keeper_stats_']\"))>=2)\n",
    "\n",
    "    # create a Beautiful Soup object from the response content\n",
    "    soup = BeautifulSoup(DRIVER.page_source, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RETRIEVE FULL PAGE INFO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_match_soup_info(match_soup):\n",
    "    team_uids = get_teams_playing_uids(match_soup)\n",
    "    outfield_df_whole = get_outfield_perf(team_uids, match_soup)\n",
    "    keeper_df_whole = get_keeper_perf(team_uids, match_soup)\n",
    "    return [outfield_df_whole, keeper_df_whole]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "match_soup = load_match_soup(MATCH_REPORT_URL, DRIVER)\n",
    "outfield_df_whole, keeper_df_whole = retrieve_match_soup_info(match_soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fpl-scraper-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
